{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('DataFrame') \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.53.70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=DataFrame>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie DataFrame'u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './SparkSQLdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# json usuniecie data_path+\n",
    "people = spark.read.json(data_path+'people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "people = spark.read.json(data_path+'people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv usuniecie data_path z people_txt = spark.read.option(\"inferSchema\", \"true\").csv(data_path+'people.txt')\n",
    "people_txt = spark.read.option(\"inferSchema\", \"true\").csv('people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|    _c0| _c1|\n",
      "+-------+----+\n",
      "|Michael|29.0|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolekcja Row'ów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson1 = Row(name='Greg', age=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=32, name='Greg')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Greg', 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson1.name, newPerson1.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson1['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'age' in newPerson1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson = Row(\"age\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson2 = newPerson(24, 'Alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=24, name='Alice')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPerson2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPerson3 = newPerson(None, None)\n",
    "newPerson4 = newPerson(33, None)\n",
    "newPerson5 = newPerson(None, 'Peter')\n",
    "newPerson6 = newPerson(32, 'Peter')\n",
    "newPerson7 = newPerson(40, 'Greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPeopleDF = spark.createDataFrame([newPerson1, newPerson2, newPerson3, newPerson4, \n",
    "                                     newPerson5, newPerson6, newPerson7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|  32| Greg|\n",
      "|  24|Alice|\n",
      "|null| null|\n",
      "|  33| null|\n",
      "|null|Peter|\n",
      "|  32|Peter|\n",
      "|  40| Greg|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPeopleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inne lokalne kolekcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typy danych: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types\n",
    "\n",
    "Kilka podstawowych: IntegerType, DoubleType, FloatType, StringType, BooleanType, NullType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definicja schematu\n",
    "# StructType ~ Row\n",
    "schema = StructType([StructField(\"V1\", IntegerType()),\n",
    "                     StructField(\"V2\", StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lokalna kolekcja - lista list\n",
    "df = spark.createDataFrame([[1,2],[3,4]], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| V1| V2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- V1: integer (nullable = true)\n",
      " |-- V2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przechodzenie RDD <-> DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF -> RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(people.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people_rdd = people.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin'),\n",
       " Row(age=35, name='Emma'),\n",
       " Row(age=None, name=None),\n",
       " Row(age=31, name=None),\n",
       " Row(age=30, name='Michael'),\n",
       " Row(age=19, name='Eva'),\n",
       " Row(age=None, name='Emma')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin'),\n",
       " Row(age=35, name='Emma'),\n",
       " Row(age=None, name=None),\n",
       " Row(age=31, name=None),\n",
       " Row(age=30, name='Michael'),\n",
       " Row(age=19, name='Eva'),\n",
       " Row(age=None, name='Emma')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 'Michael'),\n",
       " (30, 'Andy'),\n",
       " (19, 'Justin'),\n",
       " (35, 'Emma'),\n",
       " (None, None),\n",
       " (31, None),\n",
       " (30, 'Michael'),\n",
       " (19, 'Eva'),\n",
       " (None, 'Emma')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_rdd.map(tuple).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD -> DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|  _1|     _2|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.rdd.map(tuple).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  1|\n",
      "|  1|  2|\n",
      "|  2|  3|\n",
      "|  3|  4|\n",
      "|  4|  5|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  0|  1|\n",
      "|  1|  2|\n",
      "|  2|  3|\n",
      "|  3|  4|\n",
      "|  4|  5|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do toDF można podać schemat\n",
    "schema = StructType([StructField(\"A\", IntegerType()), StructField(\"B\", StringType())])\n",
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF(schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=0, B='1'),\n",
       " Row(A=1, B='2'),\n",
       " Row(A=2, B='3'),\n",
       " Row(A=3, B='4'),\n",
       " Row(A=4, B='5')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF(schema).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(A=0, B='1')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(x, x+1) for x in range(5)]).toDF(schema).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na dwa sposoby stwórz DF z 3 wierszami i 3 kolumnami - dwie typu string, jedna numeryczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DF z recznie podanych wartosci\n",
    "# definicja schematu\n",
    "# StructType ~ Row\n",
    "schema_kolumna = StructType([StructField(\"kolumna1\", IntegerType()),\n",
    "                     StructField(\"kolumna2\", StringType()),\n",
    "                     StructField(\"kolumna3\", StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lokalna kolekcja - lista list\n",
    "df_kolumna = spark.createDataFrame([[1,2,3],[4,5,6],[7,8,9]], schema_kolumna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|kolumna1|kolumna2|kolumna3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "|       7|       8|       9|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_kolumna.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD->DF\n",
    "schema_kolumna2 = StructType([StructField(\"A\", IntegerType()),\n",
    "                              StructField(\"B\", StringType()),\n",
    "                              StructField(\"C\", StringType())])\n",
    "kz = sc.parallelize([(x, x+1,x+2) for x in range(5)]).toDF(schema_kolumna2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: integer (nullable = true)\n",
      " |-- B: string (nullable = true)\n",
      " |-- C: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kz.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  0|  1|  2|\n",
      "|  1|  2|  3|\n",
      "|  2|  3|  4|\n",
      "|  3|  4|  5|\n",
      "|  4|  5|  6|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kz.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| S1| S2| S3|\n",
      "+---+---+---+\n",
      "|  a|  b|  1|\n",
      "|  a|  b|  2|\n",
      "|  a|  b|  3|\n",
      "+---+---+---+\n",
      "\n",
      "root\n",
      " |-- S1: string (nullable = true)\n",
      " |-- S2: string (nullable = true)\n",
      " |-- S3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "constr = Row(\"S1\",\"S2\",\"S3\")\n",
    "r1 = constr(\"a\",\"b\",\"1\")\n",
    "r2 = constr(\"a\",\"b\",\"2\")\n",
    "r3 = constr(\"a\",\"b\",\"3\")\n",
    "spark.createDataFrame([r1, r2, r3]).show()\n",
    "spark.createDataFrame([r1, r2, r3]).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Z podanego RDD utwórz DF z nazwanymi kolumnami `name` i `age` oraz odpowiednimi typami (string i int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Andy, 30', 'Justin, 19']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.textFile(\"people.txt\")\n",
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"name\", StringType()),\n",
    "                    StructField(\"age\", IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRDD.map(lambda x: x.split(\", \")).map(lambda x: (x[0], int(x[1])))\\\n",
    ".toDF(schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapisywanie DataFrame'u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#people.write.csv(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.write.parquet(\"ppl_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*FileScan parquet [age#257L,name#258] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/mion/s/234/kzdunczy/Desktop/Spark/2.SQL/ppl_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,name:string>\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"ppl_parquet\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*FileScan parquet [age#281L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/mion/s/234/kzdunczy/Desktop/Spark/2.SQL/ppl_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint>\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"ppl_parquet\").select(\"age\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praca z DataFrame'ami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odwolania do poszczegolnych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista kolumn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### select(cols)\n",
    "\n",
    "Zwraca nowy DF ze wskazanymi kolumnami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "|  35|\n",
      "|null|\n",
      "|  31|\n",
      "|  30|\n",
      "|  19|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF zawierający dwie kolumny `age` (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "| age| age|\n",
      "+----+----+\n",
      "|null|null|\n",
      "|  30|  30|\n",
      "|  19|  19|\n",
      "|  35|  35|\n",
      "|null|null|\n",
      "|  31|  31|\n",
      "|  30|  30|\n",
      "|  19|  19|\n",
      "|null|null|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"age\",\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop(cols)\n",
    "\n",
    "Zwraca nowy DF bez wskazanych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|   Emma|\n",
      "|   null|\n",
      "|   null|\n",
      "|Michael|\n",
      "|    Eva|\n",
      "|   Emma|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.drop(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### withColumn(name, column)\n",
    "\n",
    "Zwraca nowy DF zawierający nowo zdefiniowaną kolumnę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "| age|   name|age+1|\n",
      "+----+-------+-----+\n",
      "|null|Michael| null|\n",
      "|  30|   Andy|   31|\n",
      "|  19| Justin|   20|\n",
      "|  35|   Emma|   36|\n",
      "|null|   null| null|\n",
      "|  31|   null|   32|\n",
      "|  30|Michael|   31|\n",
      "|  19|    Eva|   20|\n",
      "|null|   Emma| null|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(\"age+1\", people.age + 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF zawierający nową kolumnę z podwojonym wiekiem (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+\n",
      "| age|   name|twiceAge|\n",
      "+----+-------+--------+\n",
      "|null|Michael|    null|\n",
      "|  30|   Andy|      60|\n",
      "|  19| Justin|      38|\n",
      "|  35|   Emma|      70|\n",
      "|null|   null|    null|\n",
      "|  31|   null|      62|\n",
      "|  30|Michael|      60|\n",
      "|  19|    Eva|      38|\n",
      "|null|   Emma|    null|\n",
      "+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(\"twiceAge\", people.age * 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### withColumnRenamed(old, new)\n",
    "\n",
    "Zwraca nowy DF ze zmienioną nazwą jednej kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|wiek|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumnRenamed(\"age\", \"wiek\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z kolumnami o nazwach `wiek` i `imię` (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|imię|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumnRenamed(\"age\", \"wiek\")\\\n",
    ".withColumnRenamed(\"wiek\", \"imię\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filter(cond) / where(cond)\n",
    "\n",
    "Zwraca nowy DF z wierszami spełniającymi zdefiniowany warunek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 30|   Andy|\n",
      "| 35|   Emma|\n",
      "| 31|   null|\n",
      "| 30|Michael|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.filter(people.age > 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 35|   Emma|\n",
      "| 31|   null|\n",
      "| 30|Michael|\n",
      "| 19|    Eva|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.where(people.name.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 35|   Emma|\n",
      "| 31|   null|\n",
      "| 30|Michael|\n",
      "| 19|    Eva|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.where(people.age.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z wierszami w których występuje mała lub wielka litera `a` (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 30|   Andy|\n",
      "| 31|   null|\n",
      "| 30|Michael|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.filter((people.age > 20) & (people.age < 35)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  35|   Emma|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.where(people.name.contains(\"a\") | people.name.contains(\"a\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'contains(name, a)'>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.name.contains(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.dropna(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dropna(how, thresh, subset)\n",
    "\n",
    "Zwraca nowy DF z usuniętymi wierszami zawierającymi braki danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 35|   Emma|\n",
      "| 30|Michael|\n",
      "| 19|    Eva|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.dropna(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.dropna(subset=\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wykorzystując parametr `thresh` wyświetl DF z wierszami w których występuje maksymalnie jeden brak danych (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.dropna(thresh=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fillna(value, subset)\n",
    "\n",
    "Zwraca nowy DF z brakami danych zastąpionymi zdefiniowaną wartością/wartościami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|      X|\n",
      "|  31|      X|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.fillna(\"X\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "|  7|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 35|   Emma|\n",
      "|  7|      X|\n",
      "| 31|      X|\n",
      "| 30|Michael|\n",
      "| 19|    Eva|\n",
      "|  7|   Emma|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.fillna({\"age\": 7, \"name\": \"X\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na podstawie kolumny `age` stwórz kolumnę `age_filled`, wykorzystując parametr `subset` wyświetl DF w którym braki danych w kolumnie `age_filled` zastąpione są wartością 27 (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|age_filled|\n",
      "+----+-------+----------+\n",
      "|null|Michael|     11127|\n",
      "|  30|   Andy|        30|\n",
      "|  19| Justin|        19|\n",
      "|  35|   Emma|        35|\n",
      "|null|   null|     11127|\n",
      "|  31|   null|        31|\n",
      "|  30|Michael|        30|\n",
      "|  19|    Eva|        19|\n",
      "|null|   Emma|     11127|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(\"age_filled\", people.age).fillna(11127, subset=\"age_filled\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 35|   Emma|\n",
      "| 31|   null|\n",
      "| 30|Michael|\n",
      "| 19|    Eva|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.dropna(subset=\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### replace(old, new, subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "| 35|  Emma|\n",
      "| 31|  null|\n",
      "| 30|  Mick|\n",
      "| 19|   Eva|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.replace(\"Michael\", \"Mick\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "| 35|  Emma|\n",
      "| 31|  null|\n",
      "| 30|  Mick|\n",
      "| 19|   Eva|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.replace(\"Michael\", \"Mick\").where(people.age.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  31|   Andy|\n",
      "|  22| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  31|Michael|\n",
      "|  22|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.replace({30: 31, 19: 22}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na podstawie kolumny `name` stwórz kolumnę `surname`, wykorzystując parametr `subset` wyświetl DF w którym wartości w kolumnie `surname` zastąpione zostały w następujący sposób: Michael -> M, Andy -> A, itd. (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+\n",
      "| age|   name|surname|\n",
      "+----+-------+-------+\n",
      "|null|Michael|      M|\n",
      "|  30|   Andy|      A|\n",
      "|  19| Justin| Justin|\n",
      "|  35|   Emma|   Emma|\n",
      "|null|   null|   null|\n",
      "|  31|   null|   null|\n",
      "|  30|Michael|      M|\n",
      "|  19|    Eva|    Eva|\n",
      "|null|   Emma|   Emma|\n",
      "+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(\"surname\", people.name).replace({\"Michael\": \"M\", \"Andy\": \"A\"}, subset=\"surname\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### orderBy(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|null|   Emma|\n",
      "|null|   null|\n",
      "|  19| Justin|\n",
      "|  19|    Eva|\n",
      "|  30|Michael|\n",
      "|  30|   Andy|\n",
      "|  31|   null|\n",
      "|  35|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|Michael|\n",
      "|  19| Justin|\n",
      "|  19|    Eva|\n",
      "|  35|   Emma|\n",
      "|null|   Emma|\n",
      "|  30|   Andy|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.orderBy(people.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  35|   Emma|\n",
      "|  31|   null|\n",
      "|  30|   Andy|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|  19| Justin|\n",
      "|null|   null|\n",
      "|null|   Emma|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.orderBy([\"age\",\"name\"], ascending=[0,1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF zawierający uporządkowane alfabetycznie imiona osób dla których dostępna jest informacja o wieku (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 31|   null|\n",
      "| 30|   Andy|\n",
      "| 35|   Emma|\n",
      "| 19|    Eva|\n",
      "| 19| Justin|\n",
      "| 30|Michael|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.where(people.age.isNotNull()).orderBy([\"name\"], ascending=[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|   Andy|\n",
      "|   Emma|\n",
      "|    Eva|\n",
      "| Justin|\n",
      "|Michael|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.dropna().orderBy([\"name\"]).select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### union(df)\n",
    "\n",
    "Zwraca nowy DF zawierający wszystkie wiersze z dwóch łączonych DFów (odpowiednik UNION ALL w SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|    age|   name|\n",
      "+-------+-------+\n",
      "|   null|Michael|\n",
      "|     30|   Andy|\n",
      "|     19| Justin|\n",
      "|     35|   Emma|\n",
      "|   null|   null|\n",
      "|     31|   null|\n",
      "|     30|Michael|\n",
      "|     19|    Eva|\n",
      "|   null|   Emma|\n",
      "|Michael|   29.0|\n",
      "|   Andy|   30.0|\n",
      "| Justin|   19.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union sam nie dopasowuje kolumn!\n",
    "people.union(people_txt).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|30.0|   Andy|\n",
      "|19.0| Justin|\n",
      "|35.0|   Emma|\n",
      "|null|   null|\n",
      "|31.0|   null|\n",
      "|30.0|Michael|\n",
      "|19.0|    Eva|\n",
      "|null|   Emma|\n",
      "|29.0|Michael|\n",
      "|30.0|   Andy|\n",
      "|19.0| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.union(people_txt.select(sorted(people_txt.columns, reverse=True))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|null|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "|   Emma|35.0|\n",
      "|   null|null|\n",
      "|   null|31.0|\n",
      "|Michael|30.0|\n",
      "|    Eva|19.0|\n",
      "|   Emma|null|\n",
      "|Michael|29.0|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lub\n",
    "people.select(\"name\", \"age\").union(people_txt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### intersect(df)\n",
    "\n",
    "Zwraca nowy DF zawierający jedynie wiersze występujące w obydwóch DFach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  name| age|\n",
      "+------+----+\n",
      "|Justin|19.0|\n",
      "|  Andy|30.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", \"age\").intersect(people_txt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### subtract(df)\n",
    "\n",
    "Zwraca nowy DF z wierszami z oryginalnego DF które nie występują w drugim DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|30.0|\n",
      "|   null|null|\n",
      "|   Emma|35.0|\n",
      "|    Eva|19.0|\n",
      "|Michael|null|\n",
      "|   null|31.0|\n",
      "|   Emma|null|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", \"age\").subtract(people_txt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### distinct\n",
    "\n",
    "Zwraca nowy DF z unikalnymi wierszami z oryginalnego DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|30.0|\n",
      "|   null|null|\n",
      "|   Emma|35.0|\n",
      "|    Eva|19.0|\n",
      "|Michael|null|\n",
      "|Michael|29.0|\n",
      "|   null|31.0|\n",
      "| Justin|19.0|\n",
      "|   Emma|null|\n",
      "|   Andy|30.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", \"age\").union(people_txt).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dropDuplicates(subset)\n",
    "\n",
    "Zwraca nowy DF z unikalnymi wierszami z oryginalnego DF (na podstawie całych wierszy lub wskazanych kolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|30.0|\n",
      "|   null|null|\n",
      "|   Emma|35.0|\n",
      "|    Eva|19.0|\n",
      "|Michael|null|\n",
      "|Michael|29.0|\n",
      "|   null|31.0|\n",
      "| Justin|19.0|\n",
      "|   Emma|null|\n",
      "|   Andy|30.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", \"age\").union(people_txt).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|   null|\n",
      "|  19|    Eva|\n",
      "|null|Michael|\n",
      "|  35|   Emma|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.dropDuplicates(subset=[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### join(df, on, how)\n",
    "\n",
    "Zwraca nowy DF powstały na podstawie połączenia dwóch DFów w oparciu o wartości we wskazanej kolumnie/kolumnach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|   name| age| age|\n",
      "+-------+----+----+\n",
      "|   Greg|null|  32|\n",
      "|   Greg|null|  40|\n",
      "|   null|null|null|\n",
      "|   null|  31|null|\n",
      "|   null|null|null|\n",
      "|   null|null|  33|\n",
      "|    Eva|  19|null|\n",
      "|Michael|null|null|\n",
      "|Michael|  30|null|\n",
      "|  Alice|null|  24|\n",
      "|   Emma|  35|null|\n",
      "|   Emma|null|null|\n",
      "|   Andy|  30|null|\n",
      "| Justin|  19|null|\n",
      "|  Peter|null|null|\n",
      "|  Peter|null|  32|\n",
      "+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(newPeopleDF, on=\"name\", how=\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|name|age|age|\n",
      "+----+---+---+\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(newPeopleDF, on=\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+----+\n",
      "| age|   name|    _c0| _c1|\n",
      "+----+-------+-------+----+\n",
      "|null|Michael|Michael|29.0|\n",
      "|  30|   Andy|   Andy|30.0|\n",
      "|  19| Justin| Justin|19.0|\n",
      "|  30|Michael|Michael|29.0|\n",
      "+----+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(people_txt, people.name == people_txt._c0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Uwaga**\n",
    ">\n",
    ">Join to popularna, ale kosztowna operacja.<br>\n",
    ">W sytuacji, kiedy jeden z łaczonych DataFramow jest znacznie mniejszy (w szczegolnosci na tyle mały, że w całości mieści się w pamięci), zaleca sie zastosowanie *broadcast hash join*.<br>\n",
    ">(Mała tabela zostanie zebrana do pamięci i wysłana do każdego noda).<br>\n",
    ">W niektórych przypadkach optymalizator sam za nas zdecyduje o zastosowaniu *broadcast hash join*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|name|age|age|\n",
      "+----+---+---+\n",
      "|Greg| 32| 20|\n",
      "|Greg| 40| 20|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "newPeopleDF.join(broadcast(spark.createDataFrame([Row(age=20, name='Greg')])), on='name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### groupBy(cols)\n",
    "\n",
    "Zwraca nowy DF pogrupowany po wskazanej kolumnie/kolumnach (nie jest to typowy DF, nie można go podejrzeć używając show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f2ab802a7f0>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.groupBy(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   name|count|\n",
      "+-------+-----+\n",
      "|   null|    2|\n",
      "|    Eva|    1|\n",
      "|Michael|    2|\n",
      "|   Emma|    2|\n",
      "|   Andy|    1|\n",
      "| Justin|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(\"name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   name|max(age)|\n",
      "+-------+--------+\n",
      "|   null|      31|\n",
      "|    Eva|      19|\n",
      "|Michael|      30|\n",
      "|   Emma|      35|\n",
      "|   Andy|      30|\n",
      "| Justin|      19|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean/max/min\n",
    "people.groupBy(\"name\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   name|max(age)|\n",
      "+-------+--------+\n",
      "|   null|      31|\n",
      "|    Eva|      19|\n",
      "|Michael|      30|\n",
      "|   Emma|      35|\n",
      "|   Andy|      30|\n",
      "| Justin|      19|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(\"name\").max(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z informacją ile razy pojawił się każdy wiek (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    2|\n",
      "|  31|    1|\n",
      "|null|    3|\n",
      "|  35|    1|\n",
      "|  30|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### agg(expres)\n",
    "\n",
    "Zwraca nowy DF na powstały w wyniku zastosowania wskazanych agregacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   name|max(age)|\n",
      "+-------+--------+\n",
      "|   null|      31|\n",
      "|    Eva|      19|\n",
      "|Michael|      30|\n",
      "|   Emma|      35|\n",
      "|   Andy|      30|\n",
      "| Justin|      19|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(\"name\").agg(f.max(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+----------+\n",
      "|   name|max(age)|count(name)|count(age)|\n",
      "+-------+--------+-----------+----------+\n",
      "|   null|      31|          0|         1|\n",
      "|    Eva|      19|          1|         1|\n",
      "|Michael|      30|          2|         1|\n",
      "|   Emma|      35|          2|         1|\n",
      "|   Andy|      30|          1|         1|\n",
      "| Justin|      19|          1|         1|\n",
      "+-------+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(\"name\").agg(f.max(\"age\"), \n",
    "                           f.count(\"name\"), \n",
    "                           f.count(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   name|max_age|\n",
      "+-------+-------+\n",
      "|   null|     31|\n",
      "|    Eva|     19|\n",
      "|Michael|     30|\n",
      "|   Emma|     35|\n",
      "|   Andy|     30|\n",
      "| Justin|     19|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy(\"name\").agg(f.max(\"age\").alias(\"max_age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      35|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.agg(f.max(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### describe(cols)\n",
    "\n",
    "Zwraca nowy DF zawierający podstawowe statystyki wszystkich lub wskazanych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 6|      7|\n",
      "|   mean|27.333333333333332|   null|\n",
      "| stddev| 6.713171133426189|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                35|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|summary|   name|\n",
      "+-------+-------+\n",
      "|  count|      7|\n",
      "|   mean|   null|\n",
      "| stddev|   null|\n",
      "|    min|   Andy|\n",
      "|    max|Michael|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.describe(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### show(n, truncate, vertical)\n",
    "\n",
    "Wyświetla n wierszy DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "+----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+\n",
      "| age|   name|                long|\n",
      "+----+-------+--------------------+\n",
      "|null|Michael|xxxxxxxxxxxxx xxx...|\n",
      "|  30|   Andy|xxxxxxxxxxxxx xxx...|\n",
      "|  19| Justin|xxxxxxxxxxxxx xxx...|\n",
      "|  35|   Emma|xxxxxxxxxxxxx xxx...|\n",
      "+----+-------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----+-------+--------------------------------------------+\n",
      "|age |name   |long                                        |\n",
      "+----+-------+--------------------------------------------+\n",
      "|null|Michael|xxxxxxxxxxxxx xxxxxxxxxxxxxx xxxxxxxxxxxxxxx|\n",
      "|30  |Andy   |xxxxxxxxxxxxx xxxxxxxxxxxxxx xxxxxxxxxxxxxxx|\n",
      "|19  |Justin |xxxxxxxxxxxxx xxxxxxxxxxxxxx xxxxxxxxxxxxxxx|\n",
      "|35  |Emma   |xxxxxxxxxxxxx xxxxxxxxxxxxxx xxxxxxxxxxxxxxx|\n",
      "+----+-------+--------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(\"long\", f.lit(\"xxxxxxxxxxxxx xxxxxxxxxxxxxx xxxxxxxxxxxxxxx\")).show(4)\n",
    "people.withColumn(\"long\", f.lit(\"xxxxxxxxxxxxx xxxxxxxxxxxxxx xxxxxxxxxxxxxxx\")).show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collect \n",
    "\n",
    "Zwraca DF jako lokalną kolekcję (listę) wierszy \n",
    "\n",
    "**NIEBEZPIECZNE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin'),\n",
       " Row(age=35, name='Emma'),\n",
       " Row(age=None, name=None),\n",
       " Row(age=31, name=None),\n",
       " Row(age=30, name='Michael'),\n",
       " Row(age=19, name='Eva'),\n",
       " Row(age=None, name='Emma')]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin'),\n",
       " Row(age=35, name='Emma')]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.limit(4).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### take(n) / head(n)\n",
    "\n",
    "Zwraca piersze n wierszy z DF jako lokalną kolekcję (listę) wierszy\n",
    "\n",
    "**NIEBEZPIECZNE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin'),\n",
       " Row(age=35, name='Emma'),\n",
       " Row(age=None, name=None)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin'),\n",
       " Row(age=35, name='Emma'),\n",
       " Row(age=None, name=None)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael')]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first\n",
    "\n",
    "Zwraca pierwszy wiersz z DF\n",
    "\n",
    "**NIEBEZPIECZNE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count\n",
    "\n",
    "Zlicza liczbę wierszy w DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### toPandas\n",
    "\n",
    "Zwraca DF jako Pandas DF\n",
    "\n",
    "**NIEBEZPIECZNE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Michael</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Andy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Justin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>Emma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Michael</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Eva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Emma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age     name\n",
       "0   NaN  Michael\n",
       "1  30.0     Andy\n",
       "2  19.0   Justin\n",
       "3  35.0     Emma\n",
       "4   NaN     None\n",
       "5  31.0     None\n",
       "6  30.0  Michael\n",
       "7  19.0      Eva\n",
       "8   NaN     Emma"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje SQL\n",
    "\n",
    "Z funkcji tych można korzystać wewnątrz operacji: select, withColumn, agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max, min, avg, stddv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+-----------------+-----+\n",
      "|max|min|               avg|           stddev|count|\n",
      "+---+---+------------------+-----------------+-----+\n",
      "| 35| 19|27.333333333333332|6.713171133426189|    6|\n",
      "+---+---+------------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(f.max(people.age).alias(\"max\"), \n",
    "              f.min(people.age).alias(\"min\"), \n",
    "              f.avg(people.age).alias(\"avg\"), \n",
    "              f.stddev(people.age).alias(\"stddev\"), \n",
    "              f.count(people.age).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+-----------------+-----+\n",
      "|max|min|               avg|           stddev|count|\n",
      "+---+---+------------------+-----------------+-----+\n",
      "| 35| 19|27.333333333333332|6.713171133426189|    6|\n",
      "+---+---+------------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.agg(f.max(people.age).alias(\"max\"), \n",
    "           f.min(people.age).alias(\"min\"), \n",
    "           f.avg(people.age).alias(\"avg\"), \n",
    "           f.stddev(people.age).alias(\"stddev\"), \n",
    "           f.count(people.age).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lit\n",
    "\n",
    "Tworzy kolumnę ze stałą wartością"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "| age|   name|const|\n",
      "+----+-------+-----+\n",
      "|null|Michael|    7|\n",
      "|  30|   Andy|    7|\n",
      "|  19| Justin|    7|\n",
      "|  35|   Emma|    7|\n",
      "|null|   null|    7|\n",
      "|  31|   null|    7|\n",
      "|  30|Michael|    7|\n",
      "|  19|    Eva|    7|\n",
      "|null|   Emma|    7|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(\"const\", f.lit(7)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### asc/desc\n",
    "\n",
    "Pozwalają na zdefiniowanie porządek sortowania w orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|null|   Emma|\n",
      "|null|   null|\n",
      "|  19| Justin|\n",
      "|  19|    Eva|\n",
      "|  30|Michael|\n",
      "|  30|   Andy|\n",
      "|  31|   null|\n",
      "|  35|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.orderBy(f.asc(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  35|   Emma|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  19|    Eva|\n",
      "|null|Michael|\n",
      "|null|   null|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.orderBy(f.desc(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### col\n",
    "\n",
    "Pozwala na odwołanie się do kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "|  35|\n",
      "|null|\n",
      "|  31|\n",
      "|  30|\n",
      "|  19|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(f.col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-706f6a367919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do operacji na kolumnach potrzebne jest odwołanie do kolumny - nazwa nie wystarcza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpeople\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ageX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"age\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'str'"
     ]
    }
   ],
   "source": [
    "# do operacji na kolumnach potrzebne jest odwołanie do kolumny - nazwa nie wystarcza\n",
    "people.withColumn(\"X\", f.lit(3))\\\n",
    ".withColumn(\"ageX\", \"X\" * \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"X\" among (age, name);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o61.apply.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"X\" among (age, name);\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:216)\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:216)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:215)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1105)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1075)\n\tat sun.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-f1ce5b76c574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DF people nie zawiera kolumny X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpeople\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ageX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"\"\"\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"X\" among (age, name);'"
     ]
    }
   ],
   "source": [
    "# DF people nie zawiera kolumny X\n",
    "people.withColumn(\"X\", f.lit(3))\\\n",
    ".withColumn(\"ageX\", people[\"X\"] * f.col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"X\" among (age, name);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o61.apply.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"X\" among (age, name);\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:216)\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:216)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:215)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1105)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1075)\n\tat sun.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-64e4491ab702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DF people nie zawiera kolumny X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpeople\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ageX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"\"\"\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"X\" among (age, name);'"
     ]
    }
   ],
   "source": [
    "# DF people nie zawiera kolumny X\n",
    "people.withColumn(\"X\", f.lit(3))\\\n",
    ".withColumn(\"ageX\", people[\"X\"] * people[\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+----+\n",
      "| age|   name|  X|ageX|\n",
      "+----+-------+---+----+\n",
      "|null|Michael|  3|null|\n",
      "|  30|   Andy|  3|  90|\n",
      "|  19| Justin|  3|  57|\n",
      "|  35|   Emma|  3| 105|\n",
      "|null|   null|  3|null|\n",
      "|  31|   null|  3|  93|\n",
      "|  30|Michael|  3|  90|\n",
      "|  19|    Eva|  3|  57|\n",
      "|null|   Emma|  3|null|\n",
      "+----+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.withColumn(\"X\", f.lit(3)).withColumn(\"ageX\", f.col(\"X\") * f.col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               time|\n",
      "+-------------------+\n",
      "|2019-04-27 09:00:00|\n",
      "|2019-06-01 16:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('time', 'string')]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = spark.createDataFrame([('2019-04-27 09:00:00',), ('2019-06-01 16:30:00',)], ['time'])\n",
    "t_df.show()\n",
    "t_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split(str, pattern)\n",
    "\n",
    "Dzieli string na podstawie wzorca (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|splitted              |\n",
      "+----------------------+\n",
      "|[2019-04-27, 09:00:00]|\n",
      "|[2019-06-01, 16:30:00]|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df = t_df.select(f.split(\"time\", \" \").alias(\"splitted\"))\n",
    "split_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      date|    time|\n",
      "+----------+--------+\n",
      "|2019-04-27|09:00:00|\n",
      "|2019-06-01|16:30:00|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dwa sposoby na wyciąganie wartości (tworzenie nowych kolumn) z kolumny zawierającej array'e\n",
    "split_df.select(split_df.splitted[0].alias(\"date\"), split_df.splitted.getItem(1).alias(\"time\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### explode\n",
    "\n",
    "Tworzy nowy wiersz z każdego elementu arraya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('splitted', 'array<string>')]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  exploded|\n",
      "+----------+\n",
      "|2019-04-27|\n",
      "|  09:00:00|\n",
      "|2019-06-01|\n",
      "|  16:30:00|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df.select(f.explode(\"splitted\").alias(\"exploded\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### from_utc_timestamp(timestamp, tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               time|\n",
      "+-------------------+\n",
      "|2019-04-27 09:00:00|\n",
      "|2019-06-01 16:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('time', 'timestamp')]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.withColumn(\"time\", f.from_utc_timestamp(t_df.time, \"GMT\")).show()\n",
    "t_df.withColumn(\"time\", f.from_utc_timestamp(t_df.time, \"GMT\")).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|lower(name)|upper(name)|\n",
      "+-----------+-----------+\n",
      "|    michael|    MICHAEL|\n",
      "|       andy|       ANDY|\n",
      "|     justin|     JUSTIN|\n",
      "|       emma|       EMMA|\n",
      "|       null|       null|\n",
      "|       null|       null|\n",
      "|    michael|    MICHAEL|\n",
      "|        eva|        EVA|\n",
      "|       emma|       EMMA|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(f.lower(people.name), f.upper(people.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   name|length(name)|\n",
      "+-------+------------+\n",
      "|Michael|           7|\n",
      "|   Andy|           4|\n",
      "| Justin|           6|\n",
      "|   Emma|           4|\n",
      "|   null|        null|\n",
      "|   null|        null|\n",
      "|Michael|           7|\n",
      "|    Eva|           3|\n",
      "|   Emma|           4|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", f.length(f.col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### substring(str, pos, len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first_letters|\n",
      "+-------------+\n",
      "|           Mi|\n",
      "|           An|\n",
      "|           Ju|\n",
      "|           Em|\n",
      "|         null|\n",
      "|         null|\n",
      "|           Mi|\n",
      "|           Ev|\n",
      "|           Em|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(f.substring(\"name\", 1, 2).alias(\"first_letters\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z kolumną zawierającą ostatnie dwie litery imion (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first_letters|\n",
      "+-------------+\n",
      "|           el|\n",
      "|           dy|\n",
      "|           in|\n",
      "|           ma|\n",
      "|         null|\n",
      "|         null|\n",
      "|           el|\n",
      "|           va|\n",
      "|           ma|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(f.substring(\"name\", -2, 2).alias(\"first_letters\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rand(seed), randn(seed)\n",
    "\n",
    "Tworzą kolumny z losowymi wartościami z rozkładów odpowiednio: jednostajnego (0-1), normalnego (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+--------------------+\n",
      "|   name|            rand(42)|            rand(42)|            rand(1)|            randn(7)|\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+\n",
      "|Michael|  0.6661236774413726|  0.6661236774413726|0.13385709732307427| -1.2904230199480902|\n",
      "|   Andy|  0.8583151351252906|  0.8583151351252906| 0.5897562959687032|  1.0028842205245463|\n",
      "| Justin|  0.9139963682495181|  0.9139963682495181|0.01540012100242305| -1.2023777869506285|\n",
      "|   Emma|  0.8664942556157945|  0.8664942556157945|0.22569943461197162| -1.3767992982903736|\n",
      "|   null| 0.43003259523546167| 0.43003259523546167| 0.9207602095112212|-0.19014010351113017|\n",
      "|   null|  0.5578165506555315|  0.5578165506555315| 0.6222816020094926| -0.8628423790295872|\n",
      "|Michael|0.003566841203428539|0.003566841203428539| 0.1029837279488438|  0.6120130788148804|\n",
      "|    Eva| 0.12234058267065306| 0.12234058267065306| 0.6678762139023474|  0.6735684167410704|\n",
      "|   Emma| 0.31507734068168514| 0.31507734068168514|0.06748208566157787|  0.6233826295914612|\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", f.rand(42), f.rand(42),\n",
    "              f.rand(1), f.randn(7)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people.select(\"name\", f.rand(42), f.rand(42),\n",
    "              f.rand(1), f.randn(7)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-231-ca1a5293756d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-231-ca1a5293756d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    people.select(\"name\", f.round(f.rand(42), 2)              .alias(\"r1\"), f.lit(1) * random.gauss(0,1).show()\u001b[0m\n\u001b[0m                                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "people.select(\"name\", f.round(f.rand(42), 2)\\\n",
    "              .alias(\"r1\"), f.lit(1) * random.gauss(0,1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### when(cond, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name|when|\n",
      "+-------+----+\n",
      "|Michael|   X|\n",
      "|   Andy|   Y|\n",
      "| Justin|   Y|\n",
      "|   Emma|   Y|\n",
      "|   null|   Y|\n",
      "|   null|   Y|\n",
      "|Michael|   X|\n",
      "|    Eva|   Y|\n",
      "|   Emma|   Y|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", f.when(people.name.startswith(\"M\"), \"X\").otherwise(\"Y\").alias(\"when\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name|when|\n",
      "+-------+----+\n",
      "|Michael|   X|\n",
      "|   Andy|   Y|\n",
      "| Justin|   Y|\n",
      "|   Emma|   Z|\n",
      "|   null|   Y|\n",
      "|   null|   Y|\n",
      "|Michael|   X|\n",
      "|    Eva|   Z|\n",
      "|   Emma|   Z|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", \n",
    "              f.when(people.name.startswith(\"M\"), \"X\")\\\n",
    "              .when(people.name.startswith(\"E\"), \"Z\")\\\n",
    "              .otherwise(\"Y\").alias(\"when\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z kolumną zawierającą wartość `>4` gdy imię ma więcej niż 4 litery oraz `<=4` w przeciwnym przypadku (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-291-eb737206d9bb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-291-eb737206d9bb>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    .otherwise(\"None\").alias(\"flag\").show()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\", f.when(f.length(f.col(\"name\")) > 4 , \">4\")\\\n",
    "              .when(f.when(f.length(f.col(\"name\")) <= 4 , \"<=4\")))\n",
    "              .otherwise(\"None\").alias(\"flag\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "years = 6\n",
    "names = ['Alice', 'Betty', 'Chris', 'Dan', 'Greg']\n",
    "names_count = len(names)\n",
    "names = sorted(names*years)\n",
    "year = [y for y in range(2005, 2005+years)] * names_count\n",
    "starting_salary = [round(random.gauss(4000, 1000),2) for i in range(names_count)]\n",
    "salary = [0 for i in range(years * names_count)]\n",
    "salary[::years] = starting_salary\n",
    "for n in range(names_count):\n",
    "    for y in range(years-1):\n",
    "        index = (years*n+1)+y\n",
    "        salary[index] = round(salary[index-1]*(1+random.gauss(0.1,0.09)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory = spark.createDataFrame([Row(name=n, year=y, salary=s) for n,y,s in zip(names, year, salary)])\n",
    "salaryHistory = salaryHistory.filter((salaryHistory['name'] != 'Greg') | (salaryHistory['year'] != 2006))\n",
    "salaryHistory = salaryHistory.union(spark.createDataFrame([Row('Alice', 3000, 2005)])).orderBy(\"name\",\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+\n",
      "| name| salary|year|\n",
      "+-----+-------+----+\n",
      "|Alice|4404.23|2005|\n",
      "|Alice| 3000.0|2005|\n",
      "|Alice|4780.34|2006|\n",
      "|Alice|4881.72|2007|\n",
      "|Alice|5280.86|2008|\n",
      "|Alice|5976.68|2009|\n",
      "|Alice|6320.14|2010|\n",
      "|Betty|4138.01|2005|\n",
      "|Betty|4376.94|2006|\n",
      "|Betty|5117.68|2007|\n",
      "|Betty|5630.26|2008|\n",
      "|Betty|6222.57|2009|\n",
      "|Betty|6623.97|2010|\n",
      "|Chris|3601.42|2005|\n",
      "|Chris|4015.66|2006|\n",
      "|Chris|4304.73|2007|\n",
      "|Chris|4650.33|2008|\n",
      "|Chris|4932.86|2009|\n",
      "|Chris|5869.98|2010|\n",
      "|  Dan|4262.75|2005|\n",
      "|  Dan|4829.28|2006|\n",
      "|  Dan|5204.81|2007|\n",
      "|  Dan|6244.11|2008|\n",
      "|  Dan|7642.18|2009|\n",
      "|  Dan|9906.35|2010|\n",
      "| Greg|4226.89|2005|\n",
      "| Greg|4935.72|2007|\n",
      "| Greg|5274.04|2008|\n",
      "| Greg|6227.36|2009|\n",
      "| Greg|6033.33|2010|\n",
      "+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaryHistory.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje okienne (window functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Służą do obliczania agregowanych wartości w grupach definiowanych oknem (window).<br>\n",
    "Zwracają wiele rekordow (tyle ile na wejsciu w grupie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### partitionBy(cols)\n",
    "\n",
    "Konstruktor tworzący okna - podział DF ze względu na wartości w podanej kolumnie/kolumnach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funkcja.over(definicja okna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+---------+\n",
      "| name| salary|year|nameCount|\n",
      "+-----+-------+----+---------+\n",
      "|Chris|3601.42|2005|        6|\n",
      "|Chris|4015.66|2006|        6|\n",
      "|Chris|4304.73|2007|        6|\n",
      "|Chris|4650.33|2008|        6|\n",
      "|Chris|4932.86|2009|        6|\n",
      "|Chris|5869.98|2010|        6|\n",
      "| Greg|4226.89|2005|        5|\n",
      "| Greg|4935.72|2007|        5|\n",
      "| Greg|5274.04|2008|        5|\n",
      "| Greg|6227.36|2009|        5|\n",
      "| Greg|6033.33|2010|        5|\n",
      "|Betty|4138.01|2005|        6|\n",
      "|Betty|4376.94|2006|        6|\n",
      "|Betty|5117.68|2007|        6|\n",
      "|Betty|5630.26|2008|        6|\n",
      "|Betty|6222.57|2009|        6|\n",
      "|Betty|6623.97|2010|        6|\n",
      "|  Dan|4262.75|2005|        6|\n",
      "|  Dan|4829.28|2006|        6|\n",
      "|  Dan|5204.81|2007|        6|\n",
      "|  Dan|6244.11|2008|        6|\n",
      "|  Dan|7642.18|2009|        6|\n",
      "|  Dan|9906.35|2010|        6|\n",
      "|Alice|4404.23|2005|        7|\n",
      "|Alice| 3000.0|2005|        7|\n",
      "|Alice|4780.34|2006|        7|\n",
      "|Alice|4881.72|2007|        7|\n",
      "|Alice|5280.86|2008|        7|\n",
      "|Alice|5976.68|2009|        7|\n",
      "|Alice|6320.14|2010|        7|\n",
      "+-----+-------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# definicja 'okna'\n",
    "windowSpec = Window.partitionBy('name')\n",
    "\n",
    "salaryHistory.withColumn('nameCount', f.count(salaryHistory['name']).over(windowSpec)).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|count|\n",
      "+-----+-----+\n",
      "|Chris|    6|\n",
      "| Greg|    5|\n",
      "|Betty|    6|\n",
      "|  Dan|    6|\n",
      "|Alice|    7|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaryHistory.groupBy(\"name\").count().show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory.join(salaryHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z kolumną zawierającą różnicę między pensją danej osoby w konkretnym roku a średnią pensją danej osoby w analizowanym okresie (salaryHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+-----------------+-------------------+\n",
      "| name| salary|year|              avg|               diff|\n",
      "+-----+-------+----+-----------------+-------------------+\n",
      "|Chris|3601.42|2005|4562.496666666667| -961.0766666666668|\n",
      "|Chris|4015.66|2006|4562.496666666667|  -546.836666666667|\n",
      "|Chris|4304.73|2007|4562.496666666667|-257.76666666666733|\n",
      "|Chris|4650.33|2008|4562.496666666667|  87.83333333333303|\n",
      "|Chris|4932.86|2009|4562.496666666667|  370.3633333333328|\n",
      "|Chris|5869.98|2010|4562.496666666667| 1307.4833333333327|\n",
      "| Greg|4226.89|2005|5339.468000000001|-1112.5780000000004|\n",
      "| Greg|4935.72|2007|5339.468000000001| -403.7480000000005|\n",
      "| Greg|5274.04|2008|5339.468000000001|  -65.4280000000008|\n",
      "| Greg|6227.36|2009|5339.468000000001|  887.8919999999989|\n",
      "| Greg|6033.33|2010|5339.468000000001|  693.8619999999992|\n",
      "|Betty|4138.01|2005|5351.571666666667|-1213.5616666666665|\n",
      "|Betty|4376.94|2006|5351.571666666667| -974.6316666666671|\n",
      "|Betty|5117.68|2007|5351.571666666667|-233.89166666666642|\n",
      "|Betty|5630.26|2008|5351.571666666667|  278.6883333333335|\n",
      "|Betty|6222.57|2009|5351.571666666667|   870.998333333333|\n",
      "|Betty|6623.97|2010|5351.571666666667| 1272.3983333333335|\n",
      "|  Dan|4262.75|2005|6348.246666666667| -2085.496666666667|\n",
      "|  Dan|4829.28|2006|6348.246666666667|-1518.9666666666672|\n",
      "|  Dan|5204.81|2007|6348.246666666667|-1143.4366666666665|\n",
      "|  Dan|6244.11|2008|6348.246666666667|-104.13666666666722|\n",
      "|  Dan|7642.18|2009|6348.246666666667| 1293.9333333333334|\n",
      "|  Dan|9906.35|2010|6348.246666666667| 3558.1033333333335|\n",
      "|Alice|4404.23|2005|4949.138571428572|  -544.908571428572|\n",
      "|Alice| 3000.0|2005|4949.138571428572|-1949.1385714285716|\n",
      "|Alice|4780.34|2006|4949.138571428572|-168.79857142857145|\n",
      "|Alice|4881.72|2007|4949.138571428572| -67.41857142857134|\n",
      "|Alice|5280.86|2008|4949.138571428572|  331.7214285714281|\n",
      "|Alice|5976.68|2009|4949.138571428572| 1027.5414285714287|\n",
      "|Alice|6320.14|2010|4949.138571428572| 1371.0014285714287|\n",
      "+-----+-------+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# definicja 'okna'\n",
    "windowSpec = Window.partitionBy('name')\n",
    "\n",
    "salaryHistory.withColumn('avg', f.avg(salaryHistory['salary']).over(windowSpec))\\\n",
    ".withColumn(\"diff\", f.col(\"salary\") - f.col(\"avg\")).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### orderBy(cols)\n",
    "\n",
    "Definiuje wewnątrz każdego okna sortowanie w oparciu o wskazaną kolumnę/kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----+----------+-------+\n",
      "| name| salary|year|rank|dense_rank|row_num|\n",
      "+-----+-------+----+----+----------+-------+\n",
      "|Chris|3601.42|2005|   1|         1|      1|\n",
      "|Chris|4015.66|2006|   2|         2|      2|\n",
      "|Chris|4304.73|2007|   3|         3|      3|\n",
      "|Chris|4650.33|2008|   4|         4|      4|\n",
      "|Chris|4932.86|2009|   5|         5|      5|\n",
      "|Chris|5869.98|2010|   6|         6|      6|\n",
      "| Greg|4226.89|2005|   1|         1|      1|\n",
      "| Greg|4935.72|2007|   2|         2|      2|\n",
      "| Greg|5274.04|2008|   3|         3|      3|\n",
      "| Greg|6227.36|2009|   4|         4|      4|\n",
      "| Greg|6033.33|2010|   5|         5|      5|\n",
      "|Betty|4138.01|2005|   1|         1|      1|\n",
      "|Betty|4376.94|2006|   2|         2|      2|\n",
      "|Betty|5117.68|2007|   3|         3|      3|\n",
      "|Betty|5630.26|2008|   4|         4|      4|\n",
      "|Betty|6222.57|2009|   5|         5|      5|\n",
      "|Betty|6623.97|2010|   6|         6|      6|\n",
      "|  Dan|4262.75|2005|   1|         1|      1|\n",
      "|  Dan|4829.28|2006|   2|         2|      2|\n",
      "|  Dan|5204.81|2007|   3|         3|      3|\n",
      "|  Dan|6244.11|2008|   4|         4|      4|\n",
      "|  Dan|7642.18|2009|   5|         5|      5|\n",
      "|  Dan|9906.35|2010|   6|         6|      6|\n",
      "|Alice|4404.23|2005|   1|         1|      1|\n",
      "|Alice| 3000.0|2005|   1|         1|      2|\n",
      "|Alice|4780.34|2006|   3|         2|      3|\n",
      "|Alice|4881.72|2007|   4|         3|      4|\n",
      "|Alice|5280.86|2008|   5|         4|      5|\n",
      "|Alice|5976.68|2009|   6|         5|      6|\n",
      "|Alice|6320.14|2010|   7|         6|      7|\n",
      "+-----+-------+----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"name\").orderBy(\"year\")\n",
    "\n",
    "salaryHistory.withColumn(\"rank\", f.rank().over(windowSpec))\\\n",
    "             .withColumn(\"dense_rank\", f.dense_rank().over(windowSpec))\\\n",
    "             .withColumn(\"row_num\", f.row_number().over(windowSpec)).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z kolumną zawierającą różnicę w pensji rok do roku dla każdej z osób - wykorzystaj funkcję `lag` (salaryHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+-------+-------------------+\n",
      "| name| salary|year|    lag|               diff|\n",
      "+-----+-------+----+-------+-------------------+\n",
      "|Chris|3601.42|2005|   null|               null|\n",
      "|Chris|4015.66|2006|3601.42|  414.2399999999998|\n",
      "|Chris|4304.73|2007|4015.66|  289.0699999999997|\n",
      "|Chris|4650.33|2008|4304.73| 345.60000000000036|\n",
      "|Chris|4932.86|2009|4650.33| 282.52999999999975|\n",
      "|Chris|5869.98|2010|4932.86|  937.1199999999999|\n",
      "| Greg|4226.89|2005|   null|               null|\n",
      "| Greg|4935.72|2007|4226.89|  708.8299999999999|\n",
      "| Greg|5274.04|2008|4935.72|  338.3199999999997|\n",
      "| Greg|6227.36|2009|5274.04|  953.3199999999997|\n",
      "| Greg|6033.33|2010|6227.36|-194.02999999999975|\n",
      "|Betty|4138.01|2005|   null|               null|\n",
      "|Betty|4376.94|2006|4138.01| 238.92999999999938|\n",
      "|Betty|5117.68|2007|4376.94|  740.7400000000007|\n",
      "|Betty|5630.26|2008|5117.68|  512.5799999999999|\n",
      "|Betty|6222.57|2009|5630.26|  592.3099999999995|\n",
      "|Betty|6623.97|2010|6222.57| 401.40000000000055|\n",
      "|  Dan|4262.75|2005|   null|               null|\n",
      "|  Dan|4829.28|2006|4262.75|  566.5299999999997|\n",
      "|  Dan|5204.81|2007|4829.28| 375.53000000000065|\n",
      "|  Dan|6244.11|2008|5204.81| 1039.2999999999993|\n",
      "|  Dan|7642.18|2009|6244.11| 1398.0700000000006|\n",
      "|  Dan|9906.35|2010|7642.18|            2264.17|\n",
      "|Alice|4404.23|2005|   null|               null|\n",
      "|Alice| 3000.0|2005|4404.23|-1404.2299999999996|\n",
      "|Alice|4780.34|2006| 3000.0| 1780.3400000000001|\n",
      "|Alice|4881.72|2007|4780.34| 101.38000000000011|\n",
      "|Alice|5280.86|2008|4881.72|  399.1399999999994|\n",
      "|Alice|5976.68|2009|5280.86|  695.8200000000006|\n",
      "|Alice|6320.14|2010|5976.68| 343.46000000000004|\n",
      "+-----+-------+----+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"name\").orderBy(\"year\")\n",
    "\n",
    "salaryHistory\\\n",
    ".withColumn(\"lag\", f.lag(f.col(\"salary\")).over(windowSpec))\\\n",
    ".withColumn(\"diff\", f.col(\"salary\") - f.col(\"lag\")).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rangeBetween(start, end)\n",
    "\n",
    "Dodaje wewnątrz każdego okna zakres (offset) na którym zastosowana zostanie wskazana funkcja (remisy w sortowaniu traktuje jak funkcja rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------------------+\n",
      "| name| salary|year|        moving_avg|\n",
      "+-----+-------+----+------------------+\n",
      "|Chris|3601.42|2005|           3601.42|\n",
      "|Chris|4015.66|2006|           3808.54|\n",
      "|Chris|4304.73|2007|3973.9366666666665|\n",
      "|Chris|4650.33|2008|          4143.035|\n",
      "|Chris|4932.86|2009|            4301.0|\n",
      "|Chris|5869.98|2010| 4562.496666666667|\n",
      "| Greg|4226.89|2005|           4226.89|\n",
      "| Greg|4935.72|2007|          4581.305|\n",
      "| Greg|5274.04|2008| 4812.216666666667|\n",
      "| Greg|6227.36|2009|5166.0025000000005|\n",
      "| Greg|6033.33|2010| 5339.468000000001|\n",
      "|Betty|4138.01|2005|           4138.01|\n",
      "|Betty|4376.94|2006|          4257.475|\n",
      "|Betty|5117.68|2007|           4544.21|\n",
      "|Betty|5630.26|2008|         4815.7225|\n",
      "|Betty|6222.57|2009|          5097.092|\n",
      "|Betty|6623.97|2010| 5351.571666666667|\n",
      "|  Dan|4262.75|2005|           4262.75|\n",
      "|  Dan|4829.28|2006| 4546.014999999999|\n",
      "|  Dan|5204.81|2007| 4765.613333333334|\n",
      "|  Dan|6244.11|2008|         5135.2375|\n",
      "|  Dan|7642.18|2009|          5636.626|\n",
      "|  Dan|9906.35|2010| 6348.246666666667|\n",
      "|Alice|4404.23|2005|          3702.115|\n",
      "|Alice| 3000.0|2005|          3702.115|\n",
      "|Alice|4780.34|2006| 4061.523333333333|\n",
      "|Alice|4881.72|2007|         4266.5725|\n",
      "|Alice|5280.86|2008|           4469.43|\n",
      "|Alice|5976.68|2009| 4720.638333333333|\n",
      "|Alice|6320.14|2010| 4949.138571428572|\n",
      "+-----+-------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"name\").orderBy(\"year\").rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "salaryHistory.withColumn(\"moving_avg\", f.avg(f.col(\"salary\")).over(windowSpec)).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------------------+\n",
      "| name| salary|year|        moving_sum|\n",
      "+-----+-------+----+------------------+\n",
      "|Chris|3601.42|2005|           3601.42|\n",
      "|Chris|4015.66|2006|           7617.08|\n",
      "|Chris|4304.73|2007|          11921.81|\n",
      "|Chris|4650.33|2008|          16572.14|\n",
      "|Chris|4932.86|2009|           21505.0|\n",
      "|Chris|5869.98|2010|          27374.98|\n",
      "| Greg|4226.89|2005|           4226.89|\n",
      "| Greg|4935.72|2007|           9162.61|\n",
      "| Greg|5274.04|2008|14436.650000000001|\n",
      "| Greg|6227.36|2009|20664.010000000002|\n",
      "| Greg|6033.33|2010|26697.340000000004|\n",
      "|Betty|4138.01|2005|           4138.01|\n",
      "|Betty|4376.94|2006|           8514.95|\n",
      "|Betty|5117.68|2007|13632.630000000001|\n",
      "|Betty|5630.26|2008|          19262.89|\n",
      "|Betty|6222.57|2009|          25485.46|\n",
      "|Betty|6623.97|2010|          32109.43|\n",
      "|  Dan|4262.75|2005|           4262.75|\n",
      "|  Dan|4829.28|2006| 9092.029999999999|\n",
      "|  Dan|5204.81|2007|          14296.84|\n",
      "|  Dan|6244.11|2008|          20540.95|\n",
      "|  Dan|7642.18|2009|          28183.13|\n",
      "|  Dan|9906.35|2010|          38089.48|\n",
      "|Alice|4404.23|2005|           7404.23|\n",
      "|Alice| 3000.0|2005|           7404.23|\n",
      "|Alice|4780.34|2006|          12184.57|\n",
      "|Alice|4881.72|2007|          17066.29|\n",
      "|Alice|5280.86|2008|          22347.15|\n",
      "|Alice|5976.68|2009|          28323.83|\n",
      "|Alice|6320.14|2010|          34643.97|\n",
      "+-----+-------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"name\").orderBy(\"year\").rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "salaryHistory.withColumn(\"moving_sum\", f.sum(f.col(\"salary\")).over(windowSpec)).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z kolumną zawierającą średnią wyliczaną z bieżącej, poprzedniej i kolejnej pensji dla poszczególnych osób (salaryHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------------------+\n",
      "| name| salary|year|        moving_avg|\n",
      "+-----+-------+----+------------------+\n",
      "|Chris|3601.42|2005|           3808.54|\n",
      "|Chris|4015.66|2006|3973.9366666666665|\n",
      "|Chris|4304.73|2007| 4323.573333333333|\n",
      "|Chris|4650.33|2008| 4629.306666666666|\n",
      "|Chris|4932.86|2009| 5151.056666666666|\n",
      "|Chris|5869.98|2010|           5401.42|\n",
      "| Greg|4226.89|2005|           4226.89|\n",
      "| Greg|4935.72|2007|           5104.88|\n",
      "| Greg|5274.04|2008|           5479.04|\n",
      "| Greg|6227.36|2009|           5844.91|\n",
      "| Greg|6033.33|2010| 6130.344999999999|\n",
      "|Betty|4138.01|2005|          4257.475|\n",
      "|Betty|4376.94|2006|           4544.21|\n",
      "|Betty|5117.68|2007| 5041.626666666666|\n",
      "|Betty|5630.26|2008| 5656.836666666667|\n",
      "|Betty|6222.57|2009| 6158.933333333333|\n",
      "|Betty|6623.97|2010|           6423.27|\n",
      "|  Dan|4262.75|2005| 4546.014999999999|\n",
      "|  Dan|4829.28|2006| 4765.613333333334|\n",
      "|  Dan|5204.81|2007| 5426.066666666667|\n",
      "|  Dan|6244.11|2008|            6363.7|\n",
      "|  Dan|7642.18|2009|           7930.88|\n",
      "|  Dan|9906.35|2010|          8774.265|\n",
      "|Alice|4404.23|2005| 4061.523333333333|\n",
      "|Alice| 3000.0|2005| 4061.523333333333|\n",
      "|Alice|4780.34|2006|         4266.5725|\n",
      "|Alice|4881.72|2007| 4980.973333333334|\n",
      "|Alice|5280.86|2008| 5379.753333333333|\n",
      "|Alice|5976.68|2009|5859.2266666666665|\n",
      "|Alice|6320.14|2010|           6148.41|\n",
      "+-----+-------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"name\").orderBy(\"year\")\\\n",
    ".rangeBetween(-1,1)\n",
    "\n",
    "salaryHistory\\\n",
    ".withColumn(\"moving_avg\", f.avg(f.col(\"salary\")).over(windowSpec))\\\n",
    ".show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rowsBetween(start, end)\n",
    "\n",
    "Dodaje wewnątrz każdego okna zakres (offset) na którym zastosowana zostanie wskazana funkcja (remisy w sortowaniu traktuje jak funkcja row_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------------------+\n",
      "| name| salary|year|        moving_avg|\n",
      "+-----+-------+----+------------------+\n",
      "|Chris|3601.42|2005|           3601.42|\n",
      "|Chris|4015.66|2006|           3808.54|\n",
      "|Chris|4304.73|2007|3973.9366666666665|\n",
      "|Chris|4650.33|2008|          4143.035|\n",
      "|Chris|4932.86|2009|            4301.0|\n",
      "|Chris|5869.98|2010| 4562.496666666667|\n",
      "| Greg|4226.89|2005|           4226.89|\n",
      "| Greg|4935.72|2007|          4581.305|\n",
      "| Greg|5274.04|2008| 4812.216666666667|\n",
      "| Greg|6227.36|2009|5166.0025000000005|\n",
      "| Greg|6033.33|2010| 5339.468000000001|\n",
      "|Betty|4138.01|2005|           4138.01|\n",
      "|Betty|4376.94|2006|          4257.475|\n",
      "|Betty|5117.68|2007|           4544.21|\n",
      "|Betty|5630.26|2008|         4815.7225|\n",
      "|Betty|6222.57|2009|          5097.092|\n",
      "|Betty|6623.97|2010| 5351.571666666667|\n",
      "|  Dan|4262.75|2005|           4262.75|\n",
      "|  Dan|4829.28|2006| 4546.014999999999|\n",
      "|  Dan|5204.81|2007| 4765.613333333334|\n",
      "|  Dan|6244.11|2008|         5135.2375|\n",
      "|  Dan|7642.18|2009|          5636.626|\n",
      "|  Dan|9906.35|2010| 6348.246666666667|\n",
      "|Alice|4404.23|2005|           4404.23|\n",
      "|Alice| 3000.0|2005|          3702.115|\n",
      "|Alice|4780.34|2006| 4061.523333333333|\n",
      "|Alice|4881.72|2007|         4266.5725|\n",
      "|Alice|5280.86|2008|           4469.43|\n",
      "|Alice|5976.68|2009| 4720.638333333333|\n",
      "|Alice|6320.14|2010| 4949.138571428572|\n",
      "+-----+-------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"name\").orderBy(\"year\").rowsBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "salaryHistory.withColumn(\"moving_avg\", f.avg(f.col(\"salary\")).over(windowSpec)).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z kolumną zawierającą średnią wyliczaną z dwóch poprzednich pensji dla poszczególnych osób (salaryHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+------------------+\n",
      "| name| salary|year|        moving_avg|\n",
      "+-----+-------+----+------------------+\n",
      "|Chris|3601.42|2005|              null|\n",
      "|Chris|4015.66|2006|           3601.42|\n",
      "|Chris|4304.73|2007|           3808.54|\n",
      "|Chris|4650.33|2008|          4160.195|\n",
      "|Chris|4932.86|2009|           4477.53|\n",
      "|Chris|5869.98|2010| 4791.594999999999|\n",
      "| Greg|4226.89|2005|              null|\n",
      "| Greg|4935.72|2007|           4226.89|\n",
      "| Greg|5274.04|2008|          4581.305|\n",
      "| Greg|6227.36|2009|           5104.88|\n",
      "| Greg|6033.33|2010|            5750.7|\n",
      "|Betty|4138.01|2005|              null|\n",
      "|Betty|4376.94|2006|           4138.01|\n",
      "|Betty|5117.68|2007|          4257.475|\n",
      "|Betty|5630.26|2008|4747.3099999999995|\n",
      "|Betty|6222.57|2009|           5373.97|\n",
      "|Betty|6623.97|2010|          5926.415|\n",
      "|  Dan|4262.75|2005|              null|\n",
      "|  Dan|4829.28|2006|           4262.75|\n",
      "|  Dan|5204.81|2007| 4546.014999999999|\n",
      "|  Dan|6244.11|2008|          5017.045|\n",
      "|  Dan|7642.18|2009|           5724.46|\n",
      "|  Dan|9906.35|2010|          6943.145|\n",
      "|Alice|4404.23|2005|              null|\n",
      "|Alice| 3000.0|2005|           4404.23|\n",
      "|Alice|4780.34|2006|          3702.115|\n",
      "|Alice|4881.72|2007|           3890.17|\n",
      "|Alice|5280.86|2008| 4831.030000000001|\n",
      "|Alice|5976.68|2009|           5081.29|\n",
      "|Alice|6320.14|2010|           5628.77|\n",
      "+-----+-------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"name\").orderBy(\"year\")\\\n",
    ".rowsBetween(-2,-1)\n",
    "\n",
    "salaryHistory\\\n",
    ".withColumn(\"moving_avg\", f.avg(f.col(\"salary\")).over(windowSpec))\\\n",
    ".show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL wspiera standard SQL 2003\n",
    "\n",
    "Aby użyć DF w zapytaniu SQL musimy go najpierw zarejestrować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory.createOrReplaceTempView(\"salaryHistory\")\n",
    "# salaryHistory.registerTempTable(\"salaryHistorySQL\") - Spark 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proste zapytanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+\n",
      "| name| salary|year|\n",
      "+-----+-------+----+\n",
      "|Alice|4404.23|2005|\n",
      "|Alice| 3000.0|2005|\n",
      "+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from salaryHistory limit 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapytanie zwraca nowy DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark.sql(\"select * from salaryHistory limit 2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapytanie z wykorzystaniem GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "| name|          avg_sal|\n",
      "+-----+-----------------+\n",
      "|Chris|4562.496666666667|\n",
      "| Greg|5339.468000000001|\n",
      "|Betty|5351.571666666667|\n",
      "|  Dan|6348.246666666667|\n",
      "|Alice|4949.138571428572|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, avg(salary) avg_sal from salaryHistory group by name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapytanie z wykorzystaniem funkcji okiennej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------------+\n",
      "| name| salary|          avg_sal|\n",
      "+-----+-------+-----------------+\n",
      "|Chris|3601.42|4562.496666666667|\n",
      "|Chris|4015.66|4562.496666666667|\n",
      "|Chris|4304.73|4562.496666666667|\n",
      "|Chris|4650.33|4562.496666666667|\n",
      "|Chris|4932.86|4562.496666666667|\n",
      "|Chris|5869.98|4562.496666666667|\n",
      "| Greg|4226.89|5339.468000000001|\n",
      "| Greg|4935.72|5339.468000000001|\n",
      "| Greg|5274.04|5339.468000000001|\n",
      "| Greg|6227.36|5339.468000000001|\n",
      "| Greg|6033.33|5339.468000000001|\n",
      "|Betty|4138.01|5351.571666666667|\n",
      "|Betty|4376.94|5351.571666666667|\n",
      "|Betty|5117.68|5351.571666666667|\n",
      "|Betty|5630.26|5351.571666666667|\n",
      "|Betty|6222.57|5351.571666666667|\n",
      "|Betty|6623.97|5351.571666666667|\n",
      "|  Dan|4262.75|6348.246666666667|\n",
      "|  Dan|4829.28|6348.246666666667|\n",
      "|  Dan|5204.81|6348.246666666667|\n",
      "+-----+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, salary, avg(salary) over (partition by name) avg_sal from salaryHistory\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usunięcie zbioru z katalogu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"salaryHistory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Wyświetl DF z kolumną zawierającą średnią wyliczaną z dwóch poprzednich pensji dla poszczególnych osób - zadanie analogiczne do zadania przy rowsBetween - zarejestruj DF, oraz napisz odpowiednie zapytanie (salaryHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryHistory.createOrReplaceTempView(\"salaryHistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmissing 'AND' at '1'(line 1, pos 101)\\n\\n== SQL ==\\nselect name, salary, year, avg(salary) over(partition by name order by year rows between 2 preceding 1 preceding) moving_avg from salaryHistory\\n-----------------------------------------------------------------------------------------------------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmissing 'AND' at '1'(line 1, pos 101)\n\n== SQL ==\nselect name, salary, year, avg(salary) over(partition by name order by year rows between 2 preceding 1 preceding) moving_avg from salaryHistory\n-----------------------------------------------------------------------------------------------------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:217)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:114)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:68)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-306-398a614545d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmissing 'AND' at '1'(line 1, pos 101)\\n\\n== SQL ==\\nselect name, salary, year, avg(salary) over(partition by name order by year rows between 2 preceding 1 preceding) moving_avg from salaryHistory\\n-----------------------------------------------------------------------------------------------------^^^\\n\""
     ]
    }
   ],
   "source": [
    "q = (\n",
    "\"select name, salary, year, avg(salary) over\"\n",
    "\"(partition by name order by year rows between \"\n",
    "\"2 preceding 1 preceding) moving_avg from salaryHistory\"\n",
    ")\n",
    "\n",
    "spark.sql(q).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF - User Defined Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NIEWYDAJNE**\n",
    "\n",
    "Używaj tylko w ostateczności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicja funkcji pythonowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power3(value):\n",
    "    return(value**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power3(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejestracja jako UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udfPower3 = f.udf(power3, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "| salary|       power3|\n",
      "+-------+-------------+\n",
      "|4404.23| 8.5429912E10|\n",
      "| 3000.0|2.70000005E10|\n",
      "|4780.34|1.09238657E11|\n",
      "|4881.72|1.16337197E11|\n",
      "|5280.86|1.47269894E11|\n",
      "|5976.68| 2.1349122E11|\n",
      "|6320.14|2.52452749E11|\n",
      "|4138.01| 7.0855672E10|\n",
      "|4376.94| 8.3851682E10|\n",
      "|5117.68|1.34035358E11|\n",
      "|5630.26|1.78478268E11|\n",
      "|6222.57|2.40940253E11|\n",
      "|6623.97|2.90639774E11|\n",
      "|3601.42|  4.671123E10|\n",
      "|4015.66| 6.4754627E10|\n",
      "|4304.73| 7.9769666E10|\n",
      "|4650.33|1.00566032E11|\n",
      "|4932.86|1.20031814E11|\n",
      "|5869.98|2.02259939E11|\n",
      "|4262.75| 7.7458588E10|\n",
      "+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaryHistory.select(\"salary\", udfPower3(f.col(\"salary\")).alias(\"power3\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejestracja jako UDF - pozwala na stosowanie funkcji w zapytaniach SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"power3\", power3, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "| salary|       power3|\n",
      "+-------+-------------+\n",
      "|4404.23| 8.5429912E10|\n",
      "| 3000.0|2.70000005E10|\n",
      "|4780.34|1.09238657E11|\n",
      "|4881.72|1.16337197E11|\n",
      "|5280.86|1.47269894E11|\n",
      "|5976.68| 2.1349122E11|\n",
      "|6320.14|2.52452749E11|\n",
      "|4138.01| 7.0855672E10|\n",
      "|4376.94| 8.3851682E10|\n",
      "|5117.68|1.34035358E11|\n",
      "|5630.26|1.78478268E11|\n",
      "|6222.57|2.40940253E11|\n",
      "|6623.97|2.90639774E11|\n",
      "|3601.42|  4.671123E10|\n",
      "|4015.66| 6.4754627E10|\n",
      "|4304.73| 7.9769666E10|\n",
      "|4650.33|1.00566032E11|\n",
      "|4932.86|1.20031814E11|\n",
      "|5869.98|2.02259939E11|\n",
      "|4262.75| 7.7458588E10|\n",
      "+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select salary, power3(salary) power3 from salaryHistory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide(x,y):\n",
    "    return x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divide(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udfDivide = f.udf(divide, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  nonsens|\n",
      "+---------+\n",
      "|2.1966233|\n",
      "|1.4962593|\n",
      "|2.3830209|\n",
      "|2.4323468|\n",
      "|2.6299105|\n",
      "|2.9749527|\n",
      "|3.1443481|\n",
      "|2.0638454|\n",
      "|2.1819243|\n",
      "|2.5499153|\n",
      "|2.8039143|\n",
      "| 3.097347|\n",
      "|3.2955074|\n",
      "|1.7962195|\n",
      "|2.0018246|\n",
      "| 2.144858|\n",
      "|2.3159013|\n",
      "|2.4553807|\n",
      "| 2.920388|\n",
      "|2.1260598|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salaryHistory.select(udfDivide(f.col(\"salary\"), f.col(\"year\")).alias(\"nonsens\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"divide\", divide, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  nonsens|\n",
      "+---------+\n",
      "|2.1966233|\n",
      "|1.4962593|\n",
      "|2.3830209|\n",
      "|2.4323468|\n",
      "|2.6299105|\n",
      "|2.9749527|\n",
      "|3.1443481|\n",
      "|2.0638454|\n",
      "|2.1819243|\n",
      "|2.5499153|\n",
      "|2.8039143|\n",
      "| 3.097347|\n",
      "|3.2955074|\n",
      "|1.7962195|\n",
      "|2.0018246|\n",
      "| 2.144858|\n",
      "|2.3159013|\n",
      "|2.4553807|\n",
      "| 2.920388|\n",
      "|2.1260598|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select divide(salary, year) nonsens from salaryHistory\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL wspiera HiveQL. <br>\n",
    "Spark SQL wspiera rownież wczytawanie/zapisywanie danych bezpośrednio z/do Apache Hive.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiecej informacji na temat integracji z Hive:<br>\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
